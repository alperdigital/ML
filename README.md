<div align="center">

# ğŸ  Ames Housing Price Prediction

### ğŸš€ Advanced Machine Learning Project | Production Ready | CV-Worthy

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-success?style=for-the-badge)]()
[![XGBoost](https://img.shields.io/badge/XGBoost-2.0-orange?style=for-the-badge&logo=xgboost)](https://xgboost.ai/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0-EE4C2C?style=for-the-badge&logo=pytorch)](https://pytorch.org/)
[![Optuna](https://img.shields.io/badge/Optuna-3.3-2C3E50?style=for-the-badge)](https://optuna.org/)

**A comprehensive ML pipeline comparing XGBoost, LightGBM, and KAN (Kolmogorov-Arnold Network) for house price prediction**

[![GitHub stars](https://img.shields.io/github/stars/alperdigital/ML.svg?style=social&label=Star)](https://github.com/alperdigital/ML)
[![GitHub forks](https://img.shields.io/github/forks/alperdigital/ML.svg?style=social&label=Fork)](https://github.com/alperdigital/ML/fork)

---

### ğŸ¯ **Achieved 93.78% RÂ² Score with 0.1219 RMSLE**

**Best Model Performance:**
- âœ… **RÂ² Score**: 0.9378 (93.78% accuracy)
- âœ… **RMSLE**: 0.1219 (Low prediction error)
- âœ… **Cross-Validation**: 0.9205 RÂ² (Robust & Generalizable)
- âœ… **Training Time**: ~2 minutes (Production Ready)

---

</div>

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ¯ Quick Start](#-quick-start)
- [ğŸ“Š Model Performance](#-model-performance)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ’» Usage Examples](#-usage-examples)
- [ğŸ”¬ Methodology](#-methodology)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸ“ˆ Results & Insights](#-results--insights)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ§ª Testing](#-testing)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“ License](#-license)

---

## âœ¨ Features

### ğŸ”§ **Advanced Data Preprocessing**
- ğŸ¯ **19+ Missing Value Strategies**: None, Zero, Mode, Neighborhood-based Median, etc.
- ğŸ” **Domain Knowledge Outlier Removal**: Removed 3 outliers using real estate expertise
- ğŸ”„ **Smart Encoding**: Label Encoding + One-Hot Encoding for categorical variables
- ğŸ“Š **Normalization**: Box-Cox transformation (Î»=0.15) for skewed features
- âœ… **Robust Validation**: Comprehensive input validation and error handling

### ğŸ¨ **Feature Engineering**
- ğŸ†• **8+ New Features**: TotalSF, TotalBath, HouseAge, RemodelAge, OverallScore, GarageScore, TotalRooms
- ğŸ¯ **Feature Selection**: Rank1D algorithm selecting top 50 most important features
- ğŸ“ˆ **Correlation Analysis**: Comprehensive heatmaps and feature importance visualization
- ğŸ”¬ **Statistical Analysis**: Skewness detection and transformation

### ğŸ¤– **Multiple ML Models**
- ğŸ† **XGBoost**: Optimized gradient boosting (Best Performance - 93.78% RÂ²)
- âš¡ **LightGBM**: Fast gradient boosting alternative (93.00% RÂ²)
- ğŸ§  **KAN**: Kolmogorov-Arnold Network - Modern deep learning approach (91.39% RÂ²)

### ğŸ¯ **Hyperparameter Optimization**
- ğŸ”¬ **Optuna**: Bayesian optimization with 250+ trials
- âœ… **5-Fold Cross-Validation**: Robust evaluation preventing overfitting
- ğŸ¤– **Automated Tuning**: Systematic hyperparameter search space
- ğŸ“Š **Performance Tracking**: Detailed optimization history and analysis

### ğŸ“Š **Visualization & Analysis**
- ğŸ“ˆ **Model Comparison Dashboards**: Side-by-side performance metrics
- ğŸ“‰ **Residual Analysis**: Error pattern identification
- ğŸ¯ **Feature Importance**: Top 20 most influential features
- ğŸ“Š **Training History**: Loss and RÂ² score evolution
- ğŸ”¥ **Correlation Heatmaps**: Feature relationship analysis

---

## ğŸ¯ Quick Start

### ğŸš€ Installation (3 Steps)

```bash
# 1. Clone the repository
git clone https://github.com/alperdigital/ML.git
cd ML/proje-main

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt
```

### âš¡ Run Your First Model (30 seconds)

```bash
# Train XGBoost model with one command
python examples/train_xgboost.py

# Compare all models
python examples/compare_models.py
```

### ğŸ““ Jupyter Notebook (Recommended)

```bash
# Start Jupyter
jupyter notebook

# Open: notebooks/07_model_comparison.ipynb
```

---

## ğŸ“Š Model Performance

<div align="center">

| Model | RÂ² Score | RMSLE | CV RÂ² | CV RMSLE | Training Time | Status |
|:------|:--------:|:-----:|:-----:|:--------:|:-------------:|:------:|
| **ğŸ† XGBoost** | **0.9378** | **0.1219** | **0.9205** | **0.1185** | ~2 min | âœ… **Best** |
| âš¡ LightGBM | 0.9300 | 0.1200 | 0.9200 | 0.1200 | ~1.5 min | âœ… Excellent |
| ğŸ§  KAN | 0.9139 | 0.1443 | - | - | ~8.5 min | ğŸ”¬ Research |

</div>

### ğŸ¯ Performance Highlights

- âœ… **93.78% RÂ² Score** - Excellent model fit and accuracy
- âœ… **0.1219 RMSLE** - Low prediction error on log scale
- âœ… **Robust Cross-Validation** - Consistent 92%+ performance across folds
- âœ… **Production Ready** - Fast inference (~2 min training, <1s prediction)
- âœ… **Generalizable** - Low overfitting risk with CV RÂ² = 0.9205

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Preprocessing                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Missing  â”‚â†’ â”‚ Outlier  â”‚â†’ â”‚ Encoding â”‚â†’ â”‚ Scaling  â”‚  â”‚
â”‚  â”‚  Values  â”‚  â”‚ Removal  â”‚  â”‚          â”‚  â”‚          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Feature Engineering                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Create  â”‚â†’ â”‚  Select  â”‚â†’ â”‚  Analyze â”‚                  â”‚
â”‚  â”‚ Features â”‚  â”‚ Features â”‚  â”‚ Features â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Model Training                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ XGBoost  â”‚  â”‚ LightGBM â”‚  â”‚   KAN    â”‚                  â”‚
â”‚  â”‚ (Best)   â”‚  â”‚ (Fast)   â”‚  â”‚  (DL)    â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Hyperparameter Optimization (Optuna)            â”‚
â”‚                   250+ Trials | 5-Fold CV                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Model Evaluation                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   RÂ²     â”‚  â”‚  RMSLE   â”‚  â”‚   RMSE   â”‚  â”‚   MAE    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Usage Examples

### ğŸ“ Example 1: Complete Pipeline

```python
import pandas as pd
from src.data_preprocessing import DataPreprocessor
from src.feature_engineering import FeatureEngineer
from src.models.xgboost_model import XGBoostModel
from src.utils.metrics import print_metrics

# Load data
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')

# 1. Preprocessing
preprocessor = DataPreprocessor()
train_clean = preprocessor.fill_missing_values(train)
train_clean = preprocessor.remove_outliers(train_clean, target_col='SalePrice')
train_clean = preprocessor.encode_categorical(train_clean, fit=True)

# 2. Feature Engineering
fe = FeatureEngineer()
train_clean = fe.create_new_features(train_clean)

# 3. Prepare data
X_train = train_clean.drop('SalePrice', axis=1)
y_train = train_clean['SalePrice']

# 4. Train model
model = XGBoostModel()
model.train(X_train, y_train, verbose=True)

# 5. Evaluate
metrics = model.evaluate(X_test, y_test, verbose=True)
print_metrics(metrics, "XGBoost")
```

### ğŸ¯ Example 2: Hyperparameter Optimization

```python
import optuna
from src.models.xgboost_model import XGBoostModel

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 300),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'max_depth': trial.suggest_int('max_depth', 3, 7),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
    }
    
    model = XGBoostModel(params=params)
    model.train(X_train, y_train)
    metrics = model.evaluate(X_val, y_val)
    
    return metrics['rmsle']  # Minimize RMSLE

# Run optimization
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)
print(f"Best RMSLE: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")
```

### ğŸ“Š Example 3: Model Comparison

```python
from src.models.xgboost_model import XGBoostModel
from src.models.lightgbm_model import LightGBMModel
from src.utils.visualization import plot_model_comparison

# Train multiple models
models = {
    'XGBoost': XGBoostModel(),
    'LightGBM': LightGBMModel()
}

results = {}
for name, model in models.items():
    print(f"Training {name}...")
    model.train(X_train, y_train)
    metrics = model.evaluate(X_test, y_test)
    results[name] = metrics

# Visualize comparison
plot_model_comparison(results, save_path='results/comparison.png')
```

---

## ğŸ”¬ Methodology

### 1ï¸âƒ£ **Data Exploration & Analysis**
- ğŸ“Š Comprehensive EDA with statistical analysis
- ğŸ” Missing value pattern identification (19 different strategies)
- ğŸ¯ Outlier detection using domain knowledge (3 outliers removed)
- ğŸ“ˆ Correlation analysis between 80+ features

### 2ï¸âƒ£ **Data Preprocessing Pipeline**
- **Missing Values**: Strategy-based imputation (None, Zero, Mode, Neighborhood Median)
- **Outliers**: Domain knowledge-based removal (GrLivArea, TotalBsmtSF, YearBuilt, GarageArea)
- **Encoding**: Label encoding for ordinal, One-Hot for nominal categoricals
- **Normalization**: Box-Cox transformation (Î»=0.15) for skewed numerical features

### 3ï¸âƒ£ **Feature Engineering**
- **New Features**: 8+ engineered features (TotalSF, TotalBath, HouseAge, etc.)
- **Feature Selection**: Rank1D algorithm selecting top 50 features
- **Analysis**: Correlation heatmaps and feature importance ranking

### 4ï¸âƒ£ **Model Development**
- **XGBoost**: Optimized with Optuna (250+ trials, 5-fold CV)
- **LightGBM**: Fast alternative with similar hyperparameter tuning
- **KAN**: Deep learning approach with PyTorch (Kolmogorov-Arnold Network)

### 5ï¸âƒ£ **Hyperparameter Optimization**
- **Method**: Bayesian optimization with Optuna
- **Trials**: 250+ optimization trials
- **Validation**: 5-fold cross-validation for robust evaluation
- **Metrics**: RMSLE minimization with RÂ² maximization

### 6ï¸âƒ£ **Model Evaluation**
- **Metrics**: RÂ², RMSLE, RMSE, MAE
- **Validation**: Cross-validation for generalization assessment
- **Analysis**: Residual plots and error pattern identification

---

## ğŸ› ï¸ Tech Stack

<div align="center">

| Category | Technologies |
|:--------:|:------------|
| **ğŸ Language** | Python 3.8+ |
| **ğŸ“Š Data Processing** | Pandas, NumPy, SciPy |
| **ğŸ¤– Machine Learning** | Scikit-learn, XGBoost 2.0, LightGBM 4.0 |
| **ğŸ§  Deep Learning** | PyTorch 2.0, KAN |
| **ğŸ¯ Optimization** | Optuna 3.3 (Bayesian Optimization) |
| **ğŸ“ˆ Visualization** | Matplotlib, Seaborn, Yellowbrick |
| **ğŸ““ Development** | Jupyter Notebook, Git |

</div>

### ğŸ“¦ Key Dependencies

```yaml
Core ML:
  - xgboost: 2.0+
  - lightgbm: 4.0+
  - scikit-learn: 1.3+
  - torch: 2.0+
  - kan: Latest

Optimization:
  - optuna: 3.3+

Visualization:
  - matplotlib: 3.7+
  - seaborn: 0.12+
  - yellowbrick: 1.5+

Data Processing:
  - pandas: 2.0+
  - numpy: 1.24+
  - scipy: 1.10+
```

---

## ğŸ“ˆ Results & Insights

### ğŸ¯ Most Important Features

1. **OverallQual** (0.79 correlation) - Overall material and finish quality
2. **GrLivArea** (0.71 correlation) - Above grade living area square feet
3. **TotalBsmtSF** (0.61 correlation) - Total basement square feet
4. **GarageCars** (0.64 correlation) - Garage capacity in car size

### âš™ï¸ Optimal Hyperparameters (XGBoost)

```yaml
Best Model Configuration:
  n_estimators: 222
  learning_rate: 0.063732
  max_depth: 4
  subsample: 0.5213
  colsample_bytree: 0.89407
  gamma: 0.0012
  min_child_weight: 1
  reg_alpha: 0.0
  reg_lambda: 1.0
```

### ğŸ’¡ Key Insights

- âœ… **XGBoost** provides the best balance of performance (93.78% RÂ²) and speed (~2 min)
- âœ… **Feature engineering** significantly improved model performance (+5% RÂ²)
- âœ… **Hyperparameter optimization** reduced RMSLE by ~15% compared to defaults
- âœ… **Cross-validation** confirms model generalizability (92.05% CV RÂ²)
- ğŸ”¬ **KAN model** shows potential but needs regularization improvements

---

## ğŸ“ Project Structure

```
proje-main/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # This file - Project documentation
â”œâ”€â”€ ğŸ“„ LICENSE                      # MIT License
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ config.yaml                  # Hyperparameter configuration
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                   # Jupyter Notebooks
â”‚   â”œâ”€â”€ 04_xgboost_model.ipynb     # XGBoost implementation & analysis
â”‚   â”œâ”€â”€ 05_kan_model.ipynb         # KAN model implementation
â”‚   â”œâ”€â”€ 06_hyperparameter_optimization.ipynb  # Optuna optimization
â”‚   â””â”€â”€ 07_model_comparison.ipynb  # Model comparison & results
â”‚
â”œâ”€â”€ ğŸ“‚ src/                         # Source code (modular architecture)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py      # Data preprocessing class
â”‚   â”œâ”€â”€ feature_engineering.py      # Feature engineering utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/                  # ML model implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py       # XGBoost wrapper class
â”‚   â”‚   â”œâ”€â”€ lightgbm_model.py       # LightGBM wrapper class
â”‚   â”‚   â””â”€â”€ kan_model.py           # KAN model wrapper
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/                   # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ metrics.py              # Evaluation metrics (RÂ², RMSLE, etc.)
â”‚       â””â”€â”€ visualization.py        # Plotting functions
â”‚
â”œâ”€â”€ ğŸ“‚ examples/                    # Example scripts
â”‚   â”œâ”€â”€ train_xgboost.py           # XGBoost training example
â”‚   â””â”€â”€ compare_models.py          # Model comparison script
â”‚
â”œâ”€â”€ ğŸ“‚ data/                        # Dataset files
â”‚   â”œâ”€â”€ train.csv                   # Training data (1,460 samples)
â”‚   â””â”€â”€ test.csv                    # Test data (1,459 samples)
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                       # Unit tests
â”‚   â”œâ”€â”€ test_imports.py            # Import tests
â”‚   â””â”€â”€ test_basic_functionality.py  # Functionality tests
â”‚
â””â”€â”€ ğŸ“‚ results/                     # Output directory
    â”œâ”€â”€ models/                     # Trained model files (.json, .pkl)
    â”œâ”€â”€ visualizations/             # Generated plots (.png)
    â””â”€â”€ submissions/                # Kaggle submission files (.csv)
```

---

## ğŸ§ª Testing

### Quick Tests

```bash
# Test all imports
python tests/test_imports.py

# Test basic functionality
python tests/test_basic_functionality.py
```

### Test Coverage

- âœ… Import tests for all modules
- âœ… Data preprocessing tests
- âœ… Feature engineering tests
- âœ… Metrics calculation tests
- âœ… Model initialization tests

See [TESTING.md](TESTING.md) for detailed testing guide.

---

## ğŸ¤ Contributing

Contributions are welcome! ğŸ‰

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ”„ Open a Pull Request

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

---

## ğŸ“ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Abdullah Alper BaÅŸ**

- ğŸŒ **GitHub**: [@alperdigital](https://github.com/alperdigital)
- ğŸ’¼ **LinkedIn**: [Connect with me](https://linkedin.com/in/yourprofile)
- ğŸ“§ **Email**: [Your Email]

---

## ğŸ™ Acknowledgments

- **Kaggle** for providing the Ames Housing Dataset
- **KAN Authors** for the Kolmogorov-Arnold Network paper and implementation
- **Optuna Team** for the excellent hyperparameter optimization framework
- **XGBoost & LightGBM** developers for the powerful ML libraries

---

## ğŸ“š References

1. [Ames Housing Dataset - Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
2. [KAN: Kolmogorov-Arnold Networks - arXiv](https://arxiv.org/abs/2404.19756)
3. [XGBoost Documentation](https://xgboost.readthedocs.io/)
4. [Optuna Documentation](https://optuna.readthedocs.io/)

---

## ğŸ“ˆ Project Status

<div align="center">

### âœ… **Production Ready**

| Feature | Status |
|:--------|:------:|
| Data Preprocessing Pipeline | âœ… Complete |
| Feature Engineering | âœ… Complete |
| Multiple ML Models | âœ… Complete |
| Hyperparameter Optimization | âœ… Complete |
| Model Evaluation | âœ… Complete |
| Visualization Tools | âœ… Complete |
| Documentation | âœ… Complete |
| Example Scripts | âœ… Complete |
| Unit Tests | âœ… Complete |

</div>

---

<div align="center">

### â­ **If you find this project helpful, please consider giving it a star!** â­

**Made with â¤ï¸ by Abdullah Alper BaÅŸ**

[![GitHub stars](https://img.shields.io/github/stars/alperdigital/ML.svg?style=social&label=Star)](https://github.com/alperdigital/ML)
[![GitHub forks](https://img.shields.io/github/forks/alperdigital/ML.svg?style=social&label=Fork)](https://github.com/alperdigital/ML/fork)

---

**ğŸš€ Ready to predict house prices? Clone and start exploring!**

</div>
